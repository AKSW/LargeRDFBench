#summary How to calculate Precision, Recall, and F1 scores for a qiven SPARQL query?

This page provides details about how one can use the BigRDFBench java utility to calculate Precision, Recall, and F1 scores for a qiven SPARQL query. The main aim of this utility is to measure the completeness and correctness of results retrieved by the underlying federation engine for a given SPARQL query. First, we explain how this utility can be used with BigRDFBench, followed by the details how one can use it for any other benchmark. 

 === Using with BigRDFBench ===

 # Download the Java [http://goo.gl/8ql1UA BigRDFBench_Util.jar] and include into the underlying federation engine jar libraries. The complete source code for this tool can also be checkout from SVN link http://bigrdfbench.googlecode.com/svn/trunk/. 
 # Download the BigRDFBench actual queries [http://goo.gl/8tX1Pa results] folder and store in your computer.
 # You can compute the Precision, Recall, and F1 score by calling the function StatsGenerator.getFscores(String queryAcutalResultsFile,TupleQueryResult res) or StatsGenerator.getFscores(String queryAcutalResultsFile, ArrayList<String> currentQueryResults).
 # You can retrieve the results missed by federation engine (if any) by calling the function StatsGenerator.getMissingResults(String queryAcutalResultsFile,TupleQueryResult res) or StatsGenerator.getMissingResults(String queryAcutalResultsFile, ArrayList<String> currentQueryResults). 

A sample code (FedX based) for retrieving the Precision, Recall and F1 scores of BigRDFBench query no S2 is given below. 
			
{{{
String S2 = "SELECT ?party ?page  WHERE {  <http://dbpedia.org/resource/Barack_Obama> 
<http://dbpedia.org/ontology/party> ?party .
?x <http://data.nytimes.com/elements/topicPage> ?page .
?x <http://www.w3.org/2002/07/owl#sameAs> <http://dbpedia.org/resource/Barack_Obama> .}";

TupleQuery query = repo.getConnection().prepareTupleQuery(QueryLanguage.SPARQL, S2); 

TupleQueryResult res = query.evaluate();

System.out.println(StatsGenerator.getFscores("D://results/S2", res));
}}}

A sample output is given below. 

Precision: 1.0, Recall: 1.0, F1: 1.0

*The above steps also applied to [https://code.google.com/p/fbench/ FedBench] federation benchmark. 

=== Using with any other Benchamrk===
In order to use with other bechmarks, the following extra steps are required:
 # Store all of the benchmark queries in a directory with one file (any extension) per query.
 # Store the corresponding actual results of queries into another directory with one file (any extension) per query results in tabular format. The results must be separated by some unique character(s) (lets call it resultSeparator). You may download sample query and results files from [https://drive.google.com/file/d/0BzemFAUFXpqOUi1fVGZnOTdHS2s/edit?usp=sharing here]. Further, the query file and corresponding results file should have same name. 
 # Convert the query results into RDF NTripples by calling the function ResultsRDFizer.generateRDFResults(String queriesLocation, String resultsLocation, String outputLocation, String resultSeparator). The output file name will be results.n3
 # Repeat BigRDFBench steps explained above.

It is important to note that this tool is still at an early stage and yet not properly tested. Further more, it is highly sensitive to character encoding (UTF-8 is used) specially LD7 and C8 queries contains many rare characters which needs proper encoding. Therefore, it is highly recommended to have a special look at the Precision, Recall, and F1 values for these two queries. It is also important to note that Precision, Recall, and F1 scores are not valid if SPARQL LIMIT clause is used in the query (C1,C4,C5,C9). This is because the results comes in random order until the specified LIMIT is reached. 
If you encounter any issue/problem please use project issue tracker https://code.google.com/p/bigrdfbench/issues/list or directly contact at saleem.muhammd@gmail.com

* Precision, Recall, and F1 scores are calculated using the formula  given at http://en.wikipedia.org/wiki/F1_score