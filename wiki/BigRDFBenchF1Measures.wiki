This page provides details about how one can use the BigRDFBench java tool (BigRDFBench-F1_Measure.jar) to calculate Precision, Recall, and F1 scores for a qiven SPARQL query. The main aim of this tool is to measure the completeness and correctness of results retrieved by the underlying federation engine for a given SPARQL query. First, we explain how this tool can be used with BigRDFBench, followed by the details how one can use it for any other benchmark. 

 === Using with BigRDFBench ===

 * Download the Java [https://drive.google.com/file/d/0BzemFAUFXpqOSkx6bU9nQ1c5QlE/edit?usp=sharing BigRDFBench-F1_Measure.jar] and include into the underlying federation engine jar libraries. The complete source code for this tool can also be checkout from SVN link http://bigrdfbench.googlecode.com/svn/trunk/. 
 * Download the BigRDFBench actual [https://drive.google.com/file/d/0BzemFAUFXpqOM2xnUnJ1OUI3NGc/edit?usp=sharing results] (N3) file and store it into a directory.
 * Load the above results.n3 file into in-memory sesame index by calling the  ResultsLoader.loadResults(ResultsFileLocation) function. This is one time process and you dont  need to load every time you run a query. 
 * You can compute the Precision, Recall, and F1 score by calling the function StatsGenerator.getFscores(BigRDFBenchQueryNo, QueryResultSetIterator). 
 * You can retrieve the results missed by federation engine (if any) by calling the function StatsGenerator.getMissingResults(BigRDFBenchQueryNo, QueryResultSetIterator

A sample code (FedX based) for retrieving the Precision, Recall and F1 scores of BigRDFBench query no S2 is given below. 

String S2 = "SELECT ?party ?page  WHERE {  <http://dbpedia.org/resource/Barack_Obama> 
<http://dbpedia.org/ontology/party> ?party .
?x <http://data.nytimes.com/elements/topicPage> ?page .
?x <http://www.w3.org/2002/07/owl#sameAs> <http://dbpedia.org/resource/Barack_Obama> .}";
			

String results = "D:/BigRDFBench/completeness_correctness/results.nt"; //Location of the results.nt file

ResultsLoader.loadResults(results);

TupleQuery query = repo.getConnection().prepareTupleQuery(QueryLanguage.SPARQL, S2); 

TupleQueryResult res = query.evaluate();

System.out.println(StatsGenerator.getFscores("S2", res));

A sample output is given below. 

Precision: 1.0, Recall: 1.0, F1: 1.0

=== Using with any other Benchamrk===
In order to use with other bechmarks, the following extra steps are required:
 * Store all of the benchmark queries in a directory with one file (.csv only) per query.
 * Store the corresponding actual results of queries into another directory with one file(.csv only) per query results in tabular format. The results must be separated by some unique character(s)(lets call it resultSeparator. You may download sample query and results files from [https://drive.google.com/file/d/0BzemFAUFXpqOUi1fVGZnOTdHS2s/edit?usp=sharing here].  
 * Convert the query results into RDF NTripples by calling the function ResultsRDFizer.generateRDFResults(String queriesLocation, String resultsLocation, String outputLocation, String resultSeparator). The output file name will be results.n3
 * Repeat BigRDFBench steps explained above.

It is important to note that this tool is still at an early stage and yet not properly tested. Further more, it is highly sensitive to character encoding (UTF-8 is used) specially LD7 and C8 queries contains many rare characters which needs proper encoding. Therefore, it is highly recommended to have a special look at the Precision, Recall, and F1 values for these two queries. It is also important to note that Precision, Recall, and F1 scores are not valid if SPARQL LIMIT clause is used in the query. This is because the results comes in random order until the specified LIMIT is reached. 
If you encounter any issues/problem please using project issue tracker https://code.google.com/p/bigrdfbench/issues/list or directly contact me at saleem.muhammd@gmail.com

* Precision, Recall, and F1 scores are calculated using the formula at given at http://en.wikipedia.org/wiki/F1_score